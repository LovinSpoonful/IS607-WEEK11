---
title: "Week 11 - Document Classification"
author: "Rob Hodde"
date: "April 10, 2016"
output: html_document
---

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:  https://spamassassin.apache.org/publiccorpus/


```{r, warning=FALSE, message=FALSE}

require(RCurl)
require(XML)
require(stringr)
require(SnowballC)
library(tm)

d <- "C:/data/zips/"  #working directory
setwd(d)
sources <- c("spam","easy_ham")

for (h in 1:length(sources)){
  fl <- as.data.frame(list.files(sources[h]), stringsAsFactors = FALSE)  #get file list
  
  #loop thru each email and add it to the corpus
  n <- 1
  i <- 1
  for (i in 1:nrow(fl)){
    f <- str_c(d,sources[h], "/", fl[i,1]) # specific file
    tmp <- readLines(f) # read the email into memory
    tmp <- str_c(tmp,collapse = "") # apply reformatting
    tmp <- htmlParse(tmp, asText = TRUE) # apply reformatting
    email <- xpathSApply(tmp, "//text()", xmlValue) # apply reformatting
    if (length(email) != 0) {
      if (n == 1){
        corpus_tmp <- Corpus(VectorSource(email)) # begin the corpus
      }
      if (i > 1){
        corpus_tmp <- c(corpus_tmp,Corpus(VectorSource(email))) # append the corpus
      }
      n <- n + 1
    }
  }
  
  # remove numbers, punctuation, stop words, set to lower case, 
  # combine words with same root ("stem"), and restore to plain text
  corpus_tmp <- tm_map(corpus_tmp,removeNumbers) 
  corpus_tmp <- tm_map(corpus_tmp,str_replace_all,pattern = "[[:punct:]]", replacement = " ")
  corpus_tmp <- tm_map(corpus_tmp,removeWords, words = stopwords("en"))
  corpus_tmp <- tm_map(corpus_tmp, tolower)
  corpus_tmp <- tm_map(corpus_tmp, stemDocument)
  corpus_tmp <- tm_map(corpus_tmp, PlainTextDocument) 
  tm_filter(corpus_tmp, FUN = function(x) any(grep("free", content(x))))  # check validity- OK
  
  # create the Term Document Matrix and remove the rare words.
  # this makes the matrix MUCH smaller
  tdm_tmp <- TermDocumentMatrix(corpus_tmp, control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))
  tdm_tmp <- removeSparseTerms(tdm_tmp,1-(70/length(corpus_tmp)))
  
  #copy the temporary tdm to the permanent tdm
  if (sources[h] == "spam"){tdm_spam <- tdm_tmp}
  if (sources[h] == "easy_ham"){tdm_easy_ham <- tdm_tmp}
}

```

Now we can inspect the Term Document Matrices.  The terms are listed in the first column of each row, and each document is represented as a column.  The values in the matrix indicate the normalized frequency of occurrence of that term in that document.  


```{r, warning=FALSE, message=FALSE}
#inspect(tdm_spam[1:nrow(tdm_spam),1:8])  # shows entire list of terms
#inspect(tdm_easy_ham[1:nrow(tdm_easy_ham),1:8])

#shows abbreviated list of terms
inspect(tdm_spam[20:30,1:4]) 
inspect(tdm_easy_ham[20:30,1:4]) 

#shows terms that appear in at least 1% of all documents
findFreqTerms(tdm_spam,0.01*ncol(tdm_spam))
findFreqTerms(tdm_easy_ham,0.01*ncol(tdm_easy_ham))

#this is the part where I got stuck.
#I do not understand how to add a column to indicate spam / not spam
#and then combine the two TDM's into a single TDM

#  training <- rbind(tdm_easy_ham[,1:500], tdm_spam[,1:500])
#  testing <- rbind(tdm_easy_ham[,501:1000], tdm_spam[,501:1000])

#I tried to figure out how this works:

#attr(tdm_tmp[[1]], "Terms") <- 1


```

--------
  
--------  

I also worked through the following from the textbook, successfully importing all the UK Government press releases, applying metadata and creating a Term Document Matrix:  

```{r, eval=FALSE}


######################################

require(RCurl)
require(XML)
require(stringr)
require(SnowballC)
library(tm)

setwd("C:/Data")

all_links <- character()
new_results <- 'government/announcements?keywords=&announcement_type_option=press-releases&topics[]=all&departments[]=all&world_locations[]=all&from_date=&to_date=01%2F07%2F2010'
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")  # CAINFO = Certification of authority information


while(length(new_results)>0 ){
  new_results <- str_c("https://www.gov.uk/",new_results)  #string concatenator
  results <- getURL(new_results, cainfo = signatures)
  results_tree <- htmlParse(results)
  all_links <- c(all_links, xpathSApply(results_tree,"//li[@id]//a", xmlGetAttr, "href"))
  new_results <- xpathSApply(results_tree,"//nav[@id='show-more-documents']//li[@class='next']//a",xmlGetAttr,"href")
}

all_links[1]
length(all_links)

for (i in 1:length(all_links)){
    url <- str_c("https://www.gov.uk", all_links[i])
    tmp <- getURL(url, cainfo = signatures)
    write(tmp, str_c("Press_Releases/",i, ".html"))
}

length(list.files("Press_Releases"))
list.files("Press_Releases") [1:4]
tmp <- readLines("Press_Releases/1.html")  # creates an emormous text string (class: character) of the whole html file
tmp <- str_c(tmp,collapse = "")  #Takes out all the cr - line feeds
tmp <- htmlParse(tmp) #puts line feeds back in, without all the extras
release <- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)  # somehow it knows to pull just the content for this division "Block 4"

# to find this:  <dd class="js-hide-extra-metadata"><a class="organisation-link" href="/government/organisations/ministry-of-defence">Ministry of Defence</a></dd>
#use this:
organization <- xpathSApply(tmp, "//a[@class='organisation-link']", xmlValue)
#organization  # [1] "Ministry of Defence" "Ministry of Defence"

#looking for:  <dd><time class="date" datetime="2010-07-01T01:00:00+01:00"> 1 July 2010</time></dd> 
publication <- xpathSApply(tmp, "//time[@class='date']", xmlValue) 
#publication # [1] " 1 July 2010" " 1 July 2010"

release_corpus <- Corpus(VectorSource(release))
meta(release_corpus[[1]], "organization") <- organization[1]
meta(release_corpus[[1]], "publication") <- publication[1]

n <- 1
for (i in 2:length(list.files("Press_Releases"))){
  
  tmp <- readLines(str_c("Press_Releases/",i,".html"))  # creates an emormous text string (class: character) of the whole html file
  tmp <- str_c(tmp,collapse = "")  #Takes out all the cr - line feeds
  tmp <- htmlParse(tmp) #puts line feeds back in, without all the extras
  release <- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)  # somehow it knows to pull just the content for this division "Block 4"
  organization <- xpathSApply(tmp, "//a[@class='organisation-link']", xmlValue)
  publication <- xpathSApply(tmp, "//time[@class='date']", xmlValue) 
  #loop thru each press release and add it to in-memory corpus
  if (length(release) != 0) {
      n <- n + 1
      release_corpus <- c(release_corpus,Corpus(VectorSource(release)))
      meta(release_corpus[[n]], "organization") <- organization[1]
      meta(release_corpus[[n]], "publication") <- publication[1]
  }
}


length(release_corpus)

release_corpus <- tm_map(release_corpus,removeNumbers)
release_corpus <- tm_map(release_corpus,str_replace_all,pattern = "[[:punct:]]", replacement = " ")
release_corpus <- tm_map(release_corpus,removeWords, words = stopwords("en"))
release_corpus <- tm_map(release_corpus, tolower)
release_corpus <- tm_map(release_corpus, stemDocument)
release_corpus <- tm_map(release_corpus, PlainTextDocument) # "tolower" above screws up the doc type, breaks functions below. this fixes it.

tm_filter(release_corpus, FUN = function(x) any(grep("afghanistan", content(x))))

tdm <- TermDocumentMatrix(release_corpus) 
tdm <- removeSparseTerms(tdm,1-(50/length(release_corpus)))
tdm


```




